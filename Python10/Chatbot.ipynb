{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import json\n",
    "import os\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "import anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_DIR = \"papers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_papers(topic: str, max_results: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Search for papers on arXiv based on a topic and store their information.\n",
    "    \n",
    "    Args:\n",
    "        topic: The topic to search for\n",
    "        max_results: Maximum number of results to retrieve (default: 5)\n",
    "        \n",
    "    Returns:\n",
    "        List of paper IDs found in the search\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use arxiv to find the papers \n",
    "    client = arxiv.Client()\n",
    "\n",
    "    # Search for the most relevant articles matching the queried topic\n",
    "    search = arxiv.Search(\n",
    "        query = topic,\n",
    "        max_results = max_results,\n",
    "        sort_by = arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "\n",
    "    papers = client.results(search)\n",
    "    \n",
    "    # Create directory for this topic\n",
    "    path = os.path.join(PAPER_DIR, topic.lower().replace(\" \", \"_\"))\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    file_path = os.path.join(path, \"papers_info.json\")\n",
    "\n",
    "    # Try to load existing papers info\n",
    "    try:\n",
    "        with open(file_path, \"r\") as json_file:\n",
    "            papers_info = json.load(json_file)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        papers_info = {}\n",
    "\n",
    "    # Process each paper and add to papers_info  \n",
    "    paper_ids = []\n",
    "    for paper in papers:\n",
    "        paper_ids.append(paper.get_short_id())\n",
    "        paper_info = {\n",
    "            'title': paper.title,\n",
    "            'authors': [author.name for author in paper.authors],\n",
    "            'summary': paper.summary,\n",
    "            'pdf_url': paper.pdf_url,\n",
    "            'published': str(paper.published.date())\n",
    "        }\n",
    "        papers_info[paper.get_short_id()] = paper_info\n",
    "    \n",
    "    # Save updated papers_info to json file\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(papers_info, json_file, indent=2)\n",
    "    \n",
    "    print(f\"Results are saved in: {file_path}\")\n",
    "    \n",
    "    return paper_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are saved in: papers/chaeeun_ryu/papers_info.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2407.20234v1',\n",
       " '2407.06506v2',\n",
       " '2504.13674v1',\n",
       " '2505.02722v1',\n",
       " '2305.18952v5']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_papers(\"Chaeeun Ryu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(paper_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Search for information about a specific paper across all topic directories.\n",
    "    \n",
    "    Args:\n",
    "        paper_id: The ID of the paper to look for\n",
    "        \n",
    "    Returns:\n",
    "        JSON string with paper information if found, error message if not found\n",
    "    \"\"\"\n",
    " \n",
    "    for item in os.listdir(PAPER_DIR):\n",
    "        item_path = os.path.join(PAPER_DIR, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            file_path = os.path.join(item_path, \"papers_info.json\")\n",
    "            if os.path.isfile(file_path):\n",
    "                try:\n",
    "                    with open(file_path, \"r\") as json_file:\n",
    "                        papers_info = json.load(json_file)\n",
    "                        if paper_id in papers_info:\n",
    "                            return json.dumps(papers_info[paper_id], indent=2)\n",
    "                except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "                    print(f\"Error reading {file_path}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    return f\"There's no saved information related to paper {paper_id}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_ids = ['2407.20234v1',\n",
    " '2407.06506v2',\n",
    " '2504.13674v1',\n",
    " '2505.02722v1',\n",
    " '2305.18952v5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information for 2407.20234v1:\n",
      "{\n",
      "  \"title\": \"Exploring Factors Affecting Student Learning Satisfaction during COVID-19 in South Korea\",\n",
      "  \"authors\": [\n",
      "    \"Jiwon Han\",\n",
      "    \"Chaeeun Ryu\",\n",
      "    \"Gayathri Nadarajan\"\n",
      "  ],\n",
      "  \"summary\": \"Understanding students' preferences and learning satisfaction during COVID-19\\nhas focused on learning attributes such as self-efficacy, performance, and\\nengagement. Although existing efforts have constructed statistical models\\ncapable of accurately identifying significant factors impacting learning\\nsatisfaction, they do not necessarily explain the complex relationships among\\nthese factors in depth. This study aimed to understand several facets related\\nto student learning preferences and satisfaction during the pandemic such as\\nindividual learner characteristics, instructional design elements and social\\nand environmental factors. Responses from 302 students from Sungkyunkwan\\nUniversity, South Korea were collected between 2021 and 2022. Information\\ngathered included their gender, study major, satisfaction and motivation levels\\nwhen learning, perceived performance, emotional state and learning environment.\\nWilcoxon Rank sum test and Explainable Boosting Machine (EBM) were performed to\\ndetermine significant differences in specific cohorts. The two core findings of\\nthe study are as follows:1) Using Wilcoxon Rank Sum test, we can attest with\\n95% confidence that students who took offline classes had significantly higher\\nlearning satisfaction, among other attributes, than those who took online\\nclasses, as with STEM versus HASS students; 2) An explainable boosting machine\\n(EBM) model fitted to 95.08% accuracy determined the top five factors affecting\\nstudents' learning satisfaction as their perceived performance, their\\nperception on participating in class activities, their study majors, their\\nability to conduct discussions in class and the study space availability at\\nhome. Positive perceived performance and ability to discuss with classmates had\\na positive impact on learning satisfaction, while negative perception on class\\nactivities participation had a negative impact on learning satisfaction.\",\n",
      "  \"pdf_url\": \"http://arxiv.org/pdf/2407.20234v1\",\n",
      "  \"published\": \"2024-07-12\"\n",
      "}\n",
      "\n",
      "Information for 2407.06506v2:\n",
      "{\n",
      "  \"title\": \"Information Seeking and Communication among International Students on Reddit\",\n",
      "  \"authors\": [\n",
      "    \"Chaeeun Han\",\n",
      "    \"Sangpil Youm\",\n",
      "    \"Hojeong Yoo\",\n",
      "    \"Sou Hyun Jang\"\n",
      "  ],\n",
      "  \"summary\": \"This study examines the impact of the COVID-19 pandemic on\\ninformation-seeking behaviors among international students, with a focus on the\\nr/f1visa subreddit. Our study indicates a considerable rise in the number of\\nusers posting more than one question during the pandemic. Those asking\\nrecurring questions demonstrate more active involvement in communication,\\nsuggesting a continuous pursuit of knowledge. Furthermore, the thematic focus\\nhas shifted from questions about jobs before COVID-19 to concerns about\\nfinances, school preparations, and taxes during COVID-19. These findings carry\\nimplications for support policymaking, highlighting the importance of\\ndelivering timely and relevant information to meet the evolving needs of\\ninternational students. To enhance international students' understanding and\\nnavigation of this dynamic environment, future research in this field is\\nnecessary.\",\n",
      "  \"pdf_url\": \"http://arxiv.org/pdf/2407.06506v2\",\n",
      "  \"published\": \"2024-07-09\"\n",
      "}\n",
      "\n",
      "Information for 2504.13674v1:\n",
      "{\n",
      "  \"title\": \"Beyond Stereotypes: Exploring How Minority College Students Experience Stigma on Reddit\",\n",
      "  \"authors\": [\n",
      "    \"Chaeeun Han\",\n",
      "    \"Sangpil Youm\",\n",
      "    \"Hojeong Yoo\",\n",
      "    \"Sou Hyun Jang\"\n",
      "  ],\n",
      "  \"summary\": \"Minority college students face unique challenges shaped by their identities\\nbased on their gender/sexual orientation, race, religion, and academic\\ninstitutions, which influence their academic and social experiences. Although\\nresearch has highlighted the challenges faced by individual minority groups,\\nthe stigma process-labeling, stereotyping, separation, status loss, and\\ndiscrimination-that underpin these experiences remains underexamined,\\nparticularly in the online spaces where college students are highly active. We\\naddress these gaps by examining posts on subreddit, r/college, as indicators\\nfor stigma processes, our approach applies a Stereotype-BERT model, including\\nstance toward each stereotype. We extend the stereotype model to encompass\\nstatus loss and discrimination by using semantic distance with their reference\\nsentences. Our analyses show that professional indicated posts are primarily\\nlabeled under the stereotyping stage, whereas posts indicating racial are\\nhighly represented in status loss and discrimination. Intersectional identified\\nposts are more frequently associated with status loss and discrimination. The\\nfindings of this study highlight the need for multifaceted intersectional\\napproaches to identifying stigma, which subsequently serve as indicators to\\npromote equity for minority groups, especially racial minorities and those\\nexperiencing compounded vulnerabilities due to intersecting identities.\",\n",
      "  \"pdf_url\": \"http://arxiv.org/pdf/2504.13674v1\",\n",
      "  \"published\": \"2025-04-18\"\n",
      "}\n",
      "\n",
      "Information for 2505.02722v1:\n",
      "{\n",
      "  \"title\": \"Enhancing LLMs' Clinical Reasoning with Real-World Data from a Nationwide Sepsis Registry\",\n",
      "  \"authors\": [\n",
      "    \"Junu Kim\",\n",
      "    \"Chaeeun Shim\",\n",
      "    \"Sungjin Park\",\n",
      "    \"Su Yeon Lee\",\n",
      "    \"Gee Young Suh\",\n",
      "    \"Chae-Man Lim\",\n",
      "    \"Seong Jin Choi\",\n",
      "    \"Song Mi Moon\",\n",
      "    \"Kyoung-Ho Song\",\n",
      "    \"Eu Suk Kim\",\n",
      "    \"Hong Bin Kim\",\n",
      "    \"Sejoong Kim\",\n",
      "    \"Chami Im\",\n",
      "    \"Dong-Wan Kang\",\n",
      "    \"Yong Soo Kim\",\n",
      "    \"Hee-Joon Bae\",\n",
      "    \"Sung Yoon Lim\",\n",
      "    \"Han-Gil Jeong\",\n",
      "    \"Edward Choi\"\n",
      "  ],\n",
      "  \"summary\": \"Although large language models (LLMs) have demonstrated impressive reasoning\\ncapabilities across general domains, their effectiveness in real-world clinical\\npractice remains limited. This is likely due to their insufficient exposure to\\nreal-world clinical data during training, as such data is typically not\\nincluded due to privacy concerns. To address this, we propose enhancing the\\nclinical reasoning capabilities of LLMs by leveraging real-world clinical data.\\nWe constructed reasoning-intensive questions from a nationwide sepsis registry\\nand fine-tuned Phi-4 on these questions using reinforcement learning, resulting\\nin C-Reason. C-Reason exhibited strong clinical reasoning capabilities on the\\nin-domain test set, as evidenced by both quantitative metrics and expert\\nevaluations. Furthermore, its enhanced reasoning capabilities generalized to a\\nsepsis dataset involving different tasks and patient cohorts, an open-ended\\nconsultations on antibiotics use task, and other diseases. Future research\\nshould focus on training LLMs with large-scale, multi-disease clinical datasets\\nto develop more powerful, general-purpose clinical reasoning models.\",\n",
      "  \"pdf_url\": \"http://arxiv.org/pdf/2505.02722v1\",\n",
      "  \"published\": \"2025-05-05\"\n",
      "}\n",
      "\n",
      "Information for 2305.18952v5:\n",
      "{\n",
      "  \"title\": \"Exploring the Practicality of Generative Retrieval on Dynamic Corpora\",\n",
      "  \"authors\": [\n",
      "    \"Chaeeun Kim\",\n",
      "    \"Soyoung Yoon\",\n",
      "    \"Hyunji Lee\",\n",
      "    \"Joel Jang\",\n",
      "    \"Sohee Yang\",\n",
      "    \"Minjoon Seo\"\n",
      "  ],\n",
      "  \"summary\": \"Benchmarking the performance of information retrieval (IR) is mostly\\nconducted with a fixed set of documents (static corpora). However, in realistic\\nscenarios, this is rarely the case and the documents to be retrieved are\\nconstantly updated and added. In this paper, we focus on Generative Retrievals\\n(GR), which apply autoregressive language models to IR problems, and explore\\ntheir adaptability and robustness in dynamic scenarios. We also conduct an\\nextensive evaluation of computational and memory efficiency, crucial factors\\nfor real-world deployment of IR systems handling vast and ever-changing\\ndocument collections. Our results on the StreamingQA benchmark demonstrate that\\nGR is more adaptable to evolving knowledge (4-11%), robust in learning\\nknowledge with temporal information, and efficient in terms of inference FLOPs\\n(x2), indexing time (x6), and storage footprint (x4) compared to Dual Encoders\\n(DE), which are commonly used in retrieval systems. Our paper highlights the\\npotential of GR for future use in practical IR systems within dynamic\\nenvironments.\",\n",
      "  \"pdf_url\": \"http://arxiv.org/pdf/2305.18952v5\",\n",
      "  \"published\": \"2023-05-27\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for paper_id in ret_ids:\n",
    "    info = extract_info(paper_id)\n",
    "    print(f\"Information for {paper_id}:\\n{info}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"name\": \"search_papers\",\n",
    "        \"description\": \"Search for papers on arXiv based on a topic and store their information.\",\n",
    "        \"input_schema\":{\n",
    "            \"type\": \"object\",\n",
    "            \"proprties\": {\n",
    "                \"topic\":{\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The topic to search for\"\n",
    "                },\n",
    "                \"max_results\":{\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"Maximum number of results to retrieve (default: 5)\",\n",
    "                    \"default\": 5\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"topic\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"extract_info\",\n",
    "        \"description\": \"Search for information about a specific paper across all topic directories.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"paper_id\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The ID of the paper to look for\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"paper_id\"]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_tool_functions = {\n",
    "    \"search_papers\": search_papers,\n",
    "    \"extract_info\": extract_info\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_tool(tool_name, tool_args):\n",
    "    result = mapping_tool_functions[tool_name](**tool_args)\n",
    "    \n",
    "    if result is None:\n",
    "        return \"The operation completed but didn't return any results.\"\n",
    "    elif isinstance(result, list):\n",
    "        return ', '.join(result)\n",
    "    elif isinstance(result, dict):\n",
    "        return json.dumps(result, indent=2)\n",
    "    else:\n",
    "        return str(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"API key is missing. Please check your .env file.\")\n",
    "else:\n",
    "    print(\"API key loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = anthropic.Anthropic(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-7'}}\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "\n",
    "# api_key = \"your_actual_api_key_here\"  # Replace with your key\n",
    "# client = anthropic.Anthropic(api_key=api_key)\n",
    "\n",
    "try:\n",
    "    response = client.messages.create(\n",
    "        max_tokens=10,\n",
    "        model=\"claude-3-7\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Test\"}]\n",
    "    )\n",
    "    print(\"Test successful:\", response)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query\n",
    "        }\n",
    "    ]\n",
    "    response = client.messages.create(max_tokens=2024,\n",
    "                                      model = \"claude-3-7-sonnet-20250219\",\n",
    "                                      tools = tools,\n",
    "                                      messages=messages)\n",
    "    \n",
    "    process_query = True\n",
    "    while process_query:\n",
    "        assistant_content = []\n",
    "        \n",
    "        for content in response.content:\n",
    "            if content.type == 'text':\n",
    "                print(content.text)\n",
    "                assistant_content.append(content)\n",
    "\n",
    "                if len(response.content) == 1:\n",
    "                    process_query = False\n",
    "            elif content.type == 'tool_use':\n",
    "                assistant_content.append(content)\n",
    "                messages.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": assistant_content\n",
    "                })\n",
    "                \n",
    "                tool_id = content.id\n",
    "                tool_args = content.input\n",
    "                tool_name = content.name\n",
    "                print(f\"Calling tool {tool_name} with args {tool_args}\")\n",
    "                \n",
    "                result = execute_tool(tool_name, tool_args)\n",
    "                messages.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\":[\n",
    "                        {\n",
    "                            \"type\": \"tool_result\",\n",
    "                            \"tool_use_id\": tool_id,\n",
    "                            \"content\": result\n",
    "                        }\n",
    "                    ]\n",
    "                })\n",
    "                response = client.messages.create(\n",
    "                    max_tokens=2024,\n",
    "                    model=\"claude-3-7-sonnet-20250219\",\n",
    "                    tools = tools,\n",
    "                    messages=messages\n",
    "                )\n",
    "                \n",
    "                if len(response.content) == 1 and response.content[0].type == 'text':\n",
    "                    print(response.content[0].text)\n",
    "                    process_query = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_loop():\n",
    "    print(\"Welcome to the Paper Search Assistant!\")\n",
    "    print(\"Type your queries or 'quit' to exit.\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"You: \")\n",
    "            if query.lower() == 'quit':\n",
    "                break\n",
    "            process_query(query)\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Paper Search Assistant!\n",
      "Type your queries or 'quit' to exit.\n",
      "Hello! I'd be happy to help you today. Is there something specific you'd like to know about? For example, I can search for scientific papers on arXiv on a particular topic or help you find information about a specific paper if you have its ID.\n",
      "\n",
      "If you're interested in research papers, just let me know what topic you're curious about, and I can search for relevant publications for you. How can I assist you today?\n",
      "\n",
      "I'd be happy to search for papers on model context protocol for you. Let me do that search right away.\n",
      "Calling tool search_papers with args {'topic': 'model context protocol'}\n",
      "Results are saved in: papers/model_context_protocol/papers_info.json\n",
      "I've found 5 papers related to \"model context protocol\". Let me extract more information about each of these papers so you can see what they're about:\n",
      "Calling tool extract_info with args {'paper_id': '2505.02279v1'}\n",
      "Calling tool extract_info with args {'paper_id': '1509.03646v1'}\n",
      "Calling tool extract_info with args {'paper_id': '2103.16325v1'}\n",
      "Calling tool extract_info with args {'paper_id': '2502.09251v1'}\n",
      "Calling tool extract_info with args {'paper_id': '1501.01657v1'}\n",
      "Based on my search, I've found several papers related to model context protocol. Here's a summary of the findings:\n",
      "\n",
      "1. **A survey of agent interoperability protocols: Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol (ANP)** (2025)\n",
      "   - This is the most directly relevant paper about Model Context Protocol (MCP)\n",
      "   - It discusses MCP as a JSON-RPC client-server interface for secure tool invocation and typed data exchange\n",
      "   - The paper compares MCP with other agent communication protocols: ACP, A2A, and ANP\n",
      "   - It proposes a phased adoption roadmap starting with MCP for tool access\n",
      "\n",
      "2. **Key Exchange Protocol in the Trusted Data Servers Context** (2015)\n",
      "   - Focuses on a Group Key Exchange protocol for Queriers and Trusted Data Servers (TDSs)\n",
      "   - Not specifically about MCP, but related to protocols in specific contexts\n",
      "\n",
      "3. **Systematic Mapping Protocol: Reasoning Algorithms on Feature Model** (2021)\n",
      "   - Discusses a protocol for conducting systematic mapping studies on feature modeling\n",
      "   - Not directly about MCP\n",
      "\n",
      "4. **Recipe: Hardware-Accelerated Replication Protocols** (2025)\n",
      "   - Focuses on transforming Crash Fault Tolerant (CFT) protocols to operate in Byzantine settings\n",
      "   - Discusses protocols in the context of modern cloud hardware\n",
      "   - Not specifically about MCP\n",
      "\n",
      "5. **A General Model for MAC Protocol Selection in Wireless Sensor Networks** (2015)\n",
      "   - Presents a model for selecting Medium Access Control (MAC) protocols for wireless sensor networks\n",
      "   - Not related to Model Context Protocol (MCP)\n",
      "\n",
      "Would you like me to search for more specific information about Model Context Protocol, or would you like more details about any of these particular papers?\n",
      "\n",
      "I need more information to help you properly. It seems you're referring to papers, but I don't have context about which papers you want me to examine.\n",
      "\n",
      "Could you please:\n",
      "1. Specify the topic you'd like me to search for papers on, or\n",
      "2. Provide the paper IDs if you already have specific papers in mind\n",
      "\n",
      "Once you provide this information, I can search for the papers and summarize them for you.\n",
      "\n",
      "I'd be happy to help you with research or information about academic papers. To get started, could you please let me know what specific topic or paper you're interested in? For example, you might want to:\n",
      "\n",
      "1. Search for recent papers on a particular topic (like quantum computing, climate change, or machine learning)\n",
      "2. Look up information about a specific paper by providing its arXiv ID\n",
      "\n",
      "Once you provide these details, I can use the available tools to find the relevant information for you.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
